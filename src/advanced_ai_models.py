
"""
Advanced AI Models for Room Classification and Spatial Analysis
"""
import numpy as np
from typing import Dict, List, Any, Tuple
import logging
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import StandardScaler
import networkx as nx

logger = logging.getLogger(__name__)

class AdvancedRoomClassifier:
    """Advanced ML-based room classification with ensemble methods"""
    
    def __init__(self):
        self.models = {
            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),
            'gradient_boosting': GradientBoostingClassifier(n_estimators=50, random_state=42),
            'neural_network': MLPClassifier(hidden_layer_sizes=(100, 50), random_state=42, max_iter=500)
        }
        self.scaler = StandardScaler()
        self.room_types = [
            'Office', 'Meeting Room', 'Reception', 'Storage', 'Bathroom', 
            'Kitchen', 'Corridor', 'Conference Room', 'Break Room', 'Server Room'
        ]
        self._initialize_models()
    
    def _initialize_models(self):
        """Initialize models with synthetic training data"""
        # Generate synthetic training features
        X_train = self._generate_training_features(1000)
        y_train = self._generate_training_labels(1000)
        
        # Scale features
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        # Train all models
        for name, model in self.models.items():
            try:
                model.fit(X_train_scaled, y_train)
                logger.info(f"Trained {name} classifier")
            except Exception as e:
                logger.warning(f"Failed to train {name}: {e}")
    
    def _generate_training_features(self, n_samples: int) -> np.ndarray:
        """Generate synthetic training features"""
        features = []
        for _ in range(n_samples):
            # Room dimensions and ratios
            area = np.random.uniform(10, 500)  # mÂ²
            aspect_ratio = np.random.uniform(0.5, 3.0)
            perimeter = np.random.uniform(20, 100)
            
            # Shape complexity
            num_corners = np.random.randint(4, 12)
            shape_complexity = num_corners / area
            
            # Position features
            x_center = np.random.uniform(0, 1000)
            y_center = np.random.uniform(0, 1000)
            
            # Connectivity features
            adjacent_rooms = np.random.randint(1, 6)
            
            features.append([
                area, aspect_ratio, perimeter, num_corners, 
                shape_complexity, x_center, y_center, adjacent_rooms
            ])
        
        return np.array(features)
    
    def _generate_training_labels(self, n_samples: int) -> np.ndarray:
        """Generate synthetic training labels based on realistic patterns"""
        labels = []
        for i in range(n_samples):
            # Use deterministic patterns based on features for consistency
            area = np.random.uniform(10, 500)
            
            if area < 20:
                room_type = np.random.choice(['Bathroom', 'Storage'])
            elif area < 50:
                room_type = np.random.choice(['Office', 'Meeting Room', 'Break Room'])
            elif area < 100:
                room_type = np.random.choice(['Office', 'Conference Room', 'Reception'])
            else:
                room_type = np.random.choice(['Conference Room', 'Reception', 'Corridor'])
            
            labels.append(room_type)
        
        return np.array(labels)
    
    def _extract_features(self, zone: Dict) -> np.ndarray:
        """Extract features from a zone for classification"""
        points = zone.get('points', [])
        if len(points) < 3:
            return np.array([0] * 8)
        
        # Calculate area
        area = self._calculate_area(points)
        
        # Calculate aspect ratio
        bounds = self._calculate_bounds(points)
        width = bounds[2] - bounds[0]
        height = bounds[3] - bounds[1]
        aspect_ratio = width / height if height > 0 else 1.0
        
        # Calculate perimeter
        perimeter = self._calculate_perimeter(points)
        
        # Number of corners
        num_corners = len(points)
        
        # Shape complexity
        shape_complexity = num_corners / area if area > 0 else 0
        
        # Centroid
        centroid = zone.get('centroid', (0, 0))
        
        # Simulate adjacent rooms
        adjacent_rooms = min(max(1, int(area / 50)), 5)  # Estimate based on area
        
        return np.array([
            area, aspect_ratio, perimeter, num_corners,
            shape_complexity, centroid[0], centroid[1], adjacent_rooms
        ])
    
    def _calculate_area(self, points: List[Tuple[float, float]]) -> float:
        """Calculate polygon area using shoelace formula"""
        if len(points) < 3:
            return 0.0
        
        area = 0.0
        n = len(points)
        for i in range(n):
            j = (i + 1) % n
            area += points[i][0] * points[j][1]
            area -= points[j][0] * points[i][1]
        return abs(area) / 2.0
    
    def _calculate_bounds(self, points: List[Tuple[float, float]]) -> Tuple[float, float, float, float]:
        """Calculate bounding box"""
        if not points:
            return (0, 0, 0, 0)
        
        x_coords = [p[0] for p in points]
        y_coords = [p[1] for p in points]
        return (min(x_coords), min(y_coords), max(x_coords), max(y_coords))
    
    def _calculate_perimeter(self, points: List[Tuple[float, float]]) -> float:
        """Calculate polygon perimeter"""
        if len(points) < 2:
            return 0.0
        
        perimeter = 0.0
        n = len(points)
        for i in range(n):
            j = (i + 1) % n
            dx = points[j][0] - points[i][0]
            dy = points[j][1] - points[i][1]
            perimeter += np.sqrt(dx*dx + dy*dy)
        return perimeter
    
    def batch_classify(self, zones: List[Dict]) -> Dict[int, Dict]:
        """Classify multiple zones using ensemble methods"""
        results = {}
        
        for i, zone in enumerate(zones):
            features = self._extract_features(zone)
            features_scaled = self.scaler.transform(features.reshape(1, -1))
            
            # Get predictions from all models
            predictions = {}
            confidences = {}
            
            for name, model in self.models.items():
                try:
                    pred = model.predict(features_scaled)[0]
                    if hasattr(model, 'predict_proba'):
                        proba = model.predict_proba(features_scaled)[0]
                        conf = np.max(proba)
                    else:
                        conf = 0.8  # Default confidence for models without probability
                    
                    predictions[name] = pred
                    confidences[name] = conf
                except Exception as e:
                    logger.warning(f"Model {name} prediction failed: {e}")
                    predictions[name] = 'Office'
                    confidences[name] = 0.5
            
            # Ensemble voting
            final_prediction = max(predictions.values(), key=list(predictions.values()).count)
            final_confidence = np.mean(list(confidences.values()))
            
            results[i] = {
                'room_type': final_prediction,
                'confidence': final_confidence,
                'model_predictions': predictions,
                'model_confidences': confidences
            }
        
        return results


class SemanticSpaceAnalyzer:
    """Analyze spatial relationships and semantic connections between rooms"""
    
    def __init__(self):
        self.adjacency_graph = None
        self.semantic_rules = self._load_semantic_rules()
    
    def _load_semantic_rules(self) -> Dict[str, Dict]:
        """Load semantic relationship rules between room types"""
        return {
            'Office': {
                'preferred_adjacent': ['Corridor', 'Reception', 'Meeting Room'],
                'avoid_adjacent': ['Bathroom', 'Kitchen'],
                'accessibility_score': 0.8
            },
            'Meeting Room': {
                'preferred_adjacent': ['Office', 'Corridor', 'Reception'],
                'avoid_adjacent': ['Bathroom', 'Storage'],
                'accessibility_score': 0.9
            },
            'Reception': {
                'preferred_adjacent': ['Office', 'Meeting Room', 'Corridor'],
                'avoid_adjacent': ['Storage', 'Server Room'],
                'accessibility_score': 1.0
            },
            'Bathroom': {
                'preferred_adjacent': ['Corridor'],
                'avoid_adjacent': ['Kitchen', 'Meeting Room'],
                'accessibility_score': 0.9
            },
            'Kitchen': {
                'preferred_adjacent': ['Break Room', 'Corridor'],
                'avoid_adjacent': ['Bathroom', 'Server Room'],
                'accessibility_score': 0.7
            },
            'Storage': {
                'preferred_adjacent': ['Corridor'],
                'avoid_adjacent': ['Reception', 'Meeting Room'],
                'accessibility_score': 0.3
            }
        }
    
    def build_space_graph(self, zones: List[Dict], analysis_results: Dict) -> nx.Graph:
        """Build a graph representing spatial relationships"""
        G = nx.Graph()
        
        # Add nodes for each zone
        for i, zone in enumerate(zones):
            room_type = analysis_results.get(i, {}).get('room_type', 'Unknown')
            centroid = zone.get('centroid', (0, 0))
            area = zone.get('area', 0)
            
            G.add_node(i, 
                      room_type=room_type,
                      centroid=centroid,
                      area=area,
                      zone_data=zone)
        
        # Add edges for adjacent rooms
        for i, zone1 in enumerate(zones):
            for j, zone2 in enumerate(zones):
                if i < j:  # Avoid duplicate edges
                    if self._are_adjacent(zone1, zone2):
                        distance = self._calculate_distance(
                            zone1.get('centroid', (0, 0)),
                            zone2.get('centroid', (0, 0))
                        )
                        G.add_edge(i, j, distance=distance, relationship='adjacent')
        
        self.adjacency_graph = G
        return G
    
    def _are_adjacent(self, zone1: Dict, zone2: Dict, threshold: float = 50.0) -> bool:
        """Check if two zones are adjacent based on proximity"""
        centroid1 = zone1.get('centroid', (0, 0))
        centroid2 = zone2.get('centroid', (0, 0))
        distance = self._calculate_distance(centroid1, centroid2)
        return distance < threshold
    
    def _calculate_distance(self, point1: Tuple[float, float], point2: Tuple[float, float]) -> float:
        """Calculate Euclidean distance between two points"""
        return np.sqrt((point1[0] - point2[0])**2 + (point1[1] - point2[1])**2)
    
    def analyze_spatial_relationships(self) -> Dict[str, Any]:
        """Analyze spatial relationships in the building"""
        if not self.adjacency_graph:
            return {}
        
        analysis = {
            'connectivity_analysis': self._analyze_connectivity(),
            'workflow_analysis': self._analyze_workflow_efficiency(),
            'accessibility_analysis': self._analyze_accessibility(),
            'semantic_compliance': self._analyze_semantic_compliance()
        }
        
        return analysis
    
    def _analyze_connectivity(self) -> Dict[str, Any]:
        """Analyze connectivity patterns"""
        G = self.adjacency_graph
        
        # Calculate connectivity metrics
        degree_centrality = nx.degree_centrality(G)
        betweenness_centrality = nx.betweenness_centrality(G)
        
        # Identify central rooms
        most_connected = max(degree_centrality, key=degree_centrality.get)
        
        return {
            'most_connected_room': most_connected,
            'average_connectivity': np.mean(list(degree_centrality.values())),
            'connectivity_distribution': degree_centrality,
            'betweenness_centrality': betweenness_centrality
        }
    
    def _analyze_workflow_efficiency(self) -> Dict[str, Any]:
        """Analyze workflow efficiency based on room placement"""
        G = self.adjacency_graph
        
        # Define common workflows
        workflows = [
            ['Reception', 'Office', 'Meeting Room'],
            ['Office', 'Break Room', 'Office'],
            ['Reception', 'Bathroom'],
            ['Office', 'Storage']
        ]
        
        workflow_scores = []
        for workflow in workflows:
            score = self._calculate_workflow_score(workflow)
            workflow_scores.append(score)
        
        return {
            'average_workflow_efficiency': np.mean(workflow_scores),
            'workflow_scores': workflow_scores,
            'bottleneck_analysis': self._identify_bottlenecks()
        }
    
    def _calculate_workflow_score(self, workflow: List[str]) -> float:
        """Calculate efficiency score for a specific workflow"""
        G = self.adjacency_graph
        total_distance = 0
        valid_paths = 0
        
        for i in range(len(workflow) - 1):
            source_rooms = [n for n, d in G.nodes(data=True) if d.get('room_type') == workflow[i]]
            target_rooms = [n for n, d in G.nodes(data=True) if d.get('room_type') == workflow[i+1]]
            
            if source_rooms and target_rooms:
                min_distance = float('inf')
                for source in source_rooms:
                    for target in target_rooms:
                        try:
                            path = nx.shortest_path(G, source, target)
                            distance = len(path) - 1  # Number of hops
                            min_distance = min(min_distance, distance)
                        except nx.NetworkXNoPath:
                            continue
                
                if min_distance != float('inf'):
                    total_distance += min_distance
                    valid_paths += 1
        
        if valid_paths == 0:
            return 0.0
        
        # Lower distance = higher efficiency
        avg_distance = total_distance / valid_paths
        return max(0, 1.0 - (avg_distance / 10.0))  # Normalize to 0-1
    
    def _identify_bottlenecks(self) -> List[int]:
        """Identify rooms that could be bottlenecks"""
        G = self.adjacency_graph
        betweenness = nx.betweenness_centrality(G)
        
        # Rooms with high betweenness centrality are potential bottlenecks
        threshold = np.mean(list(betweenness.values())) + np.std(list(betweenness.values()))
        bottlenecks = [node for node, score in betweenness.items() if score > threshold]
        
        return bottlenecks
    
    def _analyze_accessibility(self) -> Dict[str, Any]:
        """Analyze accessibility patterns"""
        G = self.adjacency_graph
        
        accessibility_scores = {}
        for node, data in G.nodes(data=True):
            room_type = data.get('room_type', 'Unknown')
            base_score = self.semantic_rules.get(room_type, {}).get('accessibility_score', 0.5)
            
            # Adjust based on connectivity
            degree = G.degree(node)
            connectivity_bonus = min(0.2, degree * 0.05)
            
            accessibility_scores[node] = min(1.0, base_score + connectivity_bonus)
        
        return {
            'room_accessibility_scores': accessibility_scores,
            'average_accessibility': np.mean(list(accessibility_scores.values())),
            'accessibility_issues': [node for node, score in accessibility_scores.items() if score < 0.6]
        }
    
    def _analyze_semantic_compliance(self) -> Dict[str, Any]:
        """Analyze compliance with semantic relationship rules"""
        G = self.adjacency_graph
        
        compliance_scores = {}
        violations = []
        
        for node, data in G.nodes(data=True):
            room_type = data.get('room_type', 'Unknown')
            if room_type not in self.semantic_rules:
                continue
            
            rules = self.semantic_rules[room_type]
            neighbors = list(G.neighbors(node))
            neighbor_types = [G.nodes[n].get('room_type', 'Unknown') for n in neighbors]
            
            # Check preferred adjacencies
            preferred_score = 0
            for preferred in rules.get('preferred_adjacent', []):
                if preferred in neighbor_types:
                    preferred_score += 1
            
            # Check avoided adjacencies
            avoid_violations = 0
            for avoid in rules.get('avoid_adjacent', []):
                if avoid in neighbor_types:
                    avoid_violations += 1
                    violations.append({
                        'room': node,
                        'room_type': room_type,
                        'violation': f"Adjacent to {avoid}",
                        'severity': 'medium'
                    })
            
            # Calculate compliance score
            total_preferred = len(rules.get('preferred_adjacent', []))
            if total_preferred > 0:
                compliance_score = (preferred_score / total_preferred) - (avoid_violations * 0.2)
                compliance_scores[node] = max(0, min(1, compliance_score))
            else:
                compliance_scores[node] = 1.0 - (avoid_violations * 0.2)
        
        return {
            'compliance_scores': compliance_scores,
            'average_compliance': np.mean(list(compliance_scores.values())) if compliance_scores else 0,
            'violations': violations,
            'total_violations': len(violations)
        }
